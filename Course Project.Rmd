---
title: "Practical Machine Learning - Weight Lifting Exercise Data"
author: "Hin Siong Chong"
date: "October 19, 2017"
output: html_document
---

## 1. Executive Summary
In this course project, I will develop a machine-learning algorithm to analyze the Weight Lifting Exercise Dataset collected from accelerometers of 6 particupants to predict the manner in which they did the exercise. The dataset is available from http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har.

## 2. Data loading and processing
### 2.1 Loading packages and data
```{r}
## load packages
library(caret)
library(rattle)
library(rpart)
library(randomForest)

## load data and treat empty values as NA
testing_raw <- read.csv("pml-testing.csv", na.strings=c("NA", ""), header=TRUE)
training_raw <- read.csv("pml-training.csv", na.strings=c("NA", ""), header=TRUE)

## explore data
dim(training_raw)
sum(complete.cases(training_raw))
dim(testing_raw)
sum(complete.cases(testing_raw))

## check identical column names of both training and testing files
colnames_training <- colnames(training_raw)
colnames_testing <- colnames(testing_raw)
all.equal(colnames_training[1:length(colnames_training)-1], colnames_testing[1:length(colnames_testing)-1])

```

### 2.2 Data cleansing  
Datasets are filtered and subsetted for data analysis
```{r} 
## remove columns with NA values
training_raw <- training_raw[, colSums(is.na(training_raw)) == 0]
testing_raw <- testing_raw[, colSums(is.na(testing_raw)) == 0]

## remove columns not needed for measurements
classe <- training_raw$classe
# for training set
training_remove <- grepl("^X|timestamp|window", names(training_raw))
training_raw <- training_raw[, !training_remove]
training_clean <- training_raw[, sapply(training_raw, is.numeric)]
training_clean$classe <- classe
# for testing set
testing_remove <- grepl("^X|timestamp|window", names(testing_raw))
testing_raw <- testing_raw[, !testing_remove]
testing_clean <- testing_raw[, sapply(testing_raw, is.numeric)]

```

### 2.3 Splitting the training set
Cleaned training dataset is split into a training subset (comprising 60% of original entries) and a testing subset (40% of original entries).
```{r}
set.seed(1122) ## for reproducibility
in_train <- createDataPartition(training_clean$classe, p=0.60, list=FALSE)
my_train <- training_clean[in_train, ]
my_test <- training_clean[-in_train, ]
dim(my_train); dim(my_test)
```

## 3. Model selection
### 3.1 rpart Model
```{r}
set.seed(1122)
modFit_Trees <- rpart(classe~., method="class", data=my_train)
fancyRpartPlot(modFit_Trees)
predict_Trees <- predict(modFit_Trees, my_test, type = "class")
CM_Trees <- confusionMatrix(predict_Trees, my_test$classe)
plot(CM_Trees$table, col = CM_Trees$byClass, main = paste("Decision Tree Confusion Matrix: Accuracy =", round(CM_Trees$overall['Accuracy'], 4)))
```
In testing this model on the testing subset, it is revealed to have an accurary of 74%. This is the most accurate for Class A and the least accurate for Class B. Out of sample error is 100-74% = 26%.

### 3.2 Random Forest Model
```{r}
set.seed(1122)
modFit_RF <- randomForest(classe ~ ., data=my_train)
predict_RF <- predict(modFit_RF, my_test, type = "class")
CM_RF <- confusionMatrix(predict_RF, my_test$classe)
plot(CM_RF$table, col = CM_Trees$byClass, main = paste("Random Forest Confusion Matrix: Accuracy =", round(CM_RF$overall['Accuracy'], 4)))
```
In testing this model on the testing subset, it is revealed to have a high accurary of 99%. This is the least accurate for Class C with a 98.8% accuracy. Out of sample error is 100-99% = 1%.

### 3.3 Generalized Boosted Regression
```{r}
set.seed(1122)
modFit_GBR <- train(classe ~ ., data=my_train, method = "gbm", trControl = trainControl(method = "repeatedcv", number = 5, repeats = 1), verbose = FALSE)
predict_GBR <- predict(modFit_GBR, newdata=my_test)
accuracy_GBR <- confusionMatrix(predict_GBR, my_test$classe)
accuracy_GBR
```
In testing this model on the testing subset, it is revealed to have a accurary of 95.7%. This is the least accurate for Class B. Out of sample error is 100-95.7% = 4.3%.

## 4. Predicting results on the test data
Based on the model selection, Random Forest model provides the highest accuracy over that of Decision Trees or Generalized Boosted Regression. Thus, Random Forest model is used as a final model to test the test data.
```{r}
final_result <- predict(modFit_RF, testing_clean, type = "class")
final_result
```


## 5. Conclusion
Random Forest is a better model for prediction of exercise quality in this project over other models such as rpart and generalized boosted regression. It produces an accuracy of 99% with an out of sample error of 1%.

